{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d1fdace-e061-45f4-bdb4-b6379c9132dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5622e2-4ed6-4e0b-81f7-d858455662b1",
   "metadata": {},
   "source": [
    "## Character-level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1eced783-480b-49e3-905e-01d954b6ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', '.', 'L', 'N', 'P', 'T', 'a', 'a', 'c', 'e', 'e', 'e', 'f', 'g', 'i', 'i', 'i', 'k', 'k', 'n', 'n', 'o', 'o', 'o', 'r', 's', 's', 't', 't', 't', 'x', 'z']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63263c52-e813-46a9-985a-aa7f712bd1a2",
   "metadata": {},
   "source": [
    "## Numericalization \n",
    "- each character needs to be converted to an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9315c64b-0630-4bcc-9e9b-1049620445ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "# Based on index postion, use enumerate\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9629bdf0-749e-42ab-9035-a69ac3a1a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 99, 'e': 101, 'r': 114, 'g': 103, 'i': 105, 'k': 107, 'n': 110, 'P': 80, 'T': 84, 's': 115, 'a': 97, 'N': 78, 'x': 120, '.': 46, 'L': 76, ' ': 32, 'f': 102, 'o': 111, 'z': 122, 't': 116}\n"
     ]
    }
   ],
   "source": [
    "# Based on Unicode, use ord\n",
    "token2uni = {ch: ord(ch) for ch in set(tokenized_text)}\n",
    "print(token2uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44f675-597b-40c9-b088-feae63867c82",
   "metadata": {},
   "source": [
    "## Transform the tokenized text to a list of integers\n",
    "\n",
    "### Based on index postion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0d5da433-03c8-4cc3-8e49-4c1fa5c84585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n",
      "38\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Based on index postion\n",
    "idx_id = []\n",
    "\n",
    "for ch in text:\n",
    "    idx_id.append(token2idx[ch])\n",
    "\n",
    "print(idx_id)\n",
    "print(len(idx_id))\n",
    "print(max(idx_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2c229-5c57-4bcc-9287-a8d3403ba2a2",
   "metadata": {},
   "source": [
    "### Based on Unicode assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a58723e0-bc76-49e5-a95e-c67c67f57bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', 'L', 'N', 'P', 'T', 'a', 'c', 'e', 'f', 'g', 'i', 'k', 'n', 'o', 'r', 's', 't', 'x', 'z']\n",
      "[32, 46, 76, 78, 80, 84, 97, 99, 101, 102, 103, 105, 107, 110, 111, 114, 115, 116, 120, 122]\n"
     ]
    }
   ],
   "source": [
    "# Sorted list of Unicode characters\n",
    "sorted_token2uni_list = sorted(list(token2uni))\n",
    "print(token2uni_list)\n",
    "# Sorted list of Unicode values\n",
    "sorted_uni_values = sorted(set(ord(ch) for ch in tokenized_text))\n",
    "print(sorted_uni_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "87438614-6d81-447b-8b59-703706779c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 111, 107, 101, 110, 105, 122, 105, 110, 103, 32, 116, 101, 120, 116, 32, 105, 115, 32, 97, 32, 99, 111, 114, 101, 32, 116, 97, 115, 107, 32, 111, 102, 32, 78, 76, 80, 46]\n",
      "38\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "# based on unicode\n",
    "uni_id = []\n",
    "\n",
    "for ch in text:\n",
    "    value = token2uni.get(ch)\n",
    "    uni_id.append(value)\n",
    "\n",
    "print(uni_id)\n",
    "print(len(uni_id))\n",
    "print(max(uni_id))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1625308-8079-404c-a150-5bb793fe5108",
   "metadata": {},
   "source": [
    "## Convert list of integers into 2D tensors with One Hot Encoding\n",
    "\n",
    "- Each row will be a one-hot vector of mostly 0s with a single 1 representing a character.\n",
    "- Each character from the original string is represented by a unique row vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f8e7dfaf-5bfc-4a73-8411-d74875f5716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0 torchvision-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d0334edf-85d6-4077-8094-4e78162a2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "eb5e942a-ca8a-446e-a07c-149a2962a241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_id = torch.tensor(idx_id)\n",
    "idx_encoded = F.one_hot(idx_id, num_classes=len(token2idx))\n",
    "idx_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7a7e37e3-8f92-4ffb-bb4b-4d2fc507eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 17\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[11]}\")\n",
    "print(f\"Tensor index: {idx_id[11]}\")\n",
    "print(f\"One-hot: {idx_encoded[11]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0bb63b6a-ebf7-467c-8849-b74433267a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32: 0, 46: 1, 76: 2, 78: 3, 80: 4, 84: 5, 97: 6, 99: 7, 101: 8, 102: 9, 103: 10, 105: 11, 107: 12, 110: 13, 111: 14, 114: 15, 115: 16, 116: 17, 120: 18, 122: 19}\n",
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "# Map the unicode assignment to 0-based indexing assignment\n",
    "uni2idx = {val: idx for idx, val in enumerate(sorted_uni_values)}\n",
    "print(uni2idx)\n",
    "# Convert uni_id to remapped index (use sorted_uni_values from prvious block)\n",
    "uni_idx_id = [uni2idx[val] for val in uni_id]\n",
    "print(uni_idx_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5c9c147e-c781-495a-be94-80dbe62d1aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_idx_id = torch.tensor(uni_idx_id)\n",
    "uni_encoded = F.one_hot(uni_idx_id, num_classes=len(uni2idx))\n",
    "uni_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ad307538-25b1-4c46-9f9d-f378f770f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 116\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[11]}\")\n",
    "print(f\"Tensor index: {uni_id[11]}\")\n",
    "print(f\"One-hot: {uni_encoded[11]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1dbf78a9-4cda-4d05-9449-8df47ad06499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"One-hot: {uni_encoded[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b7248c31-9831-4580-87b9-56ed74336345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------------------------------------------------------\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(uni_encoded[5:10])\n",
    "print('------------------------------------------------------------------------')\n",
    "print(idx_encoded[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369727c-644c-4c03-b991-3f8d09342c69",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://medium.com/@abdallahashraf90x/tokenization-in-nlp-all-you-need-to-know-45c00cfa2df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71764f78-37e7-4c9f-aea9-1fb90a6b8e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
